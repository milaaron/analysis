---
title: "Class Analysis for LING 343"
base-author: "Lisa Levinson"
student-author: "Aaron Miller"
date: 2023-03-30
format: 
  html:
    embed-resources: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE) 
here::i_am("analysis/class_analysis.qmd") 
library(here) 
library(tidyverse)
```

# Read in Data

Create dataframes from the three rds files in the data folder. 

```{r}
df_compq <- read_rds(here('data', 'df_compq.rds')) # comprehension question data
df_demo <- read_rds(here('data', 'df_demo.rds')) # demographic data
df_sprt <- read_rds(here('data', 'df_sprt.rds')) # read-time data
```

# Participant Accuracy Check

For each participant, get percent accuracy for comprehension questions.

```{r}
# My Solution: 

df_compq %>% 
  group_by(iphash, correct1) %>% 
  count() %>% 
  ungroup() %>% 
  group_by(iphash) %>% 
  pivot_wider(names_from = correct1, values_from = n) %>% 
  mutate(total = `0` + `1`) %>% 
  mutate(accuracy = `1` / total)

# Class Solution 1: 
df_compq %>% 
  group_by(iphash) %>% 
  summarize(
    correct_num = sum(correct1),
    total = n(),
    accuracy1 = correct_num/total
  ) %>% 
  mutate(accuracy =correct_num / total)


# Class Solution 2:
df_compq %>% 
  group_by(iphash) %>% 
  summarize(
    accuracy = sum(correct1)/n()
  )

```

# Combine Stimuli Information

Add the information about each stimulus sentence from the stimuli file to
the self-paced reading data. They should have matching label variables.

```{r}
stimuli <- read_csv(here('stimuli', 'stimuli-2023-eventive-repl-sprt.csv'))

df_sprt_joined <- left_join(df_sprt, 
                            select(stimuli, -sentence), 
                            join_by(label))
```

## Count Stimuli Per Condition

The first condition column in the stimuli labels the experiment. For 
the experiment labelled "gp", how many stimuli are there per condition?

```{r}
# my solution <= incorrect, cuonts total number of stimuli without categorizing
stimuli %>% 
  filter(cond1 == 'gp') %>% 
  count(sentence) %>% 
  count()

# class solution
stimuli %>% 
  filter(cond1 == 'gp') %>% 
  group_by(cond2) %>% 
  count()
```

.# A tibble: 2 × 2
.# Groups:   cond2 [2]
  cond2     n
  <chr> <int>
1 event    44
2 state    44

How many stimuli are there per item?
((should be = 2))
```{r}
stimuli %>% 
  group_by(item) %>% 
  count()
```

.# A tibble: 88 × 2
.# Groups:   item [88]
    item     n
   <dbl> <int>
 1     1    16
 2     2    16
 3     3    16
 4     4    16
 5     5    16
 6     6    16
 7     7    16
 8     8    16
 9     9    16
10    10    16
.# … with 78 more rows
.# ℹ Use `print(n = ...)` to see more rows

Answer these questions for the "mklo" stimuli. 


# McKoon and Love 2011 Replication

The "mklo" stimuli are from the study McKoon and Love 2011. They found
that result ("break") verbs had longer reaction times than manner ("hit") verbs.
The verb is the third word in each sentence. Was their effect replicated in
this experiment? Let's take some steps to see.

McKoon, G., & Love, J. (2011). Verbs in the lexicon: Why is hitting easier than breaking? Language and Cognition, 3, 313–330. <https://doi.org/10.1515/LANGCOG.2011.011>


## Summaries 

For visualization and basic summary statistics, where participants are exposed 
to repeated measures, we usually calculate averages for each participant first. 
We want the average
for each participant for each condition, so averaging "over" all of the items
they saw in that condition. But remember we want to separate the times for each
word, and we are mainly interested in the verb (word 3).

```{r}
df_sprt_joined %>% 
  filter(cond1 == 'mklo' & word_num %in% c('3')) %>% 
  group_by(iphash, cond2, word_num) %>% 
  summarize(
    mean = mean(RT, na.rm=TRUE)
  )
```


Then, we average those averages to get a condition mean. Did break verbs
take longer than hit verbs?

```{r}
df_sprt_joined %>% 
  filter(cond1 == 'mklo' & word_num %in% c('3')) %>% 
  group_by(iphash, cond2, word_num) %>% 
  summarize(
    mean = mean(RT, na.rm=TRUE)
  ) %>% 
  ungroup() %>% 
  group_by(cond2) %>% 
  summarize(
    mean2 = mean(`mean`)
  )
```

.# A tibble: 2 × 2
  cond2 mean2
  <chr> <dbl>
1 break  503.
2 hit    472.

///ANSWER: Based on the averages, yes, 'break' verbs did take ~25 milliseconds longer to process than 'hit' verbs.

Try printing your output as a formatted table by piping the dataframe/tibble
to the function `kableExtra::kbl()`. You may need to first install the
package `{kableExtra}` from CRAN. 

```{r}
df_kable_mklo <- tibble(
  df_sprt_joined %>% 
  filter(cond1 == 'mklo' & word_num %in% c('3')) %>% 
  group_by(iphash, cond2, word_num) %>% 
  summarize(
    mean = mean(RT, na.rm=TRUE)
  ) %>% 
  ungroup() %>% 
  group_by(cond2) %>% 
  summarize(
    mean2 = mean(`mean`)
  )
)

kableExtra::kbl(df_kable_mklo)
```

## Plots

Make a plot showing the means for both conditions for word 3. 

```{r}
df_kable_mklo %>% 
  ggplot(
    aes(
      x=cond2, 
      y=mean2
      ))+
  geom_col(aes(fill = cond2))+
  labs(
    title = "Mean RT for word 3",
    subtitle = "Comparing mean RT between 'break' & 'hit' verb conditions",
    y = "Mean RT (milisec.)",
    x = "Condition"
  )
  
```

Now add words 2 and 4 also. Can you put them in one faceted plot?

```{r}
tibble(
  df_sprt_joined %>% 
  filter(cond1 == 'mklo' & word_num %in% c('2','3','4')) %>% 
  group_by(iphash, cond2, word_num) %>% 
  summarize(
    mean = mean(RT, na.rm=TRUE)
  ) %>% 
  ungroup() %>% 
  group_by(cond2, word_num) %>% 
  summarize(
    mean2 = mean(`mean`)
  )
)%>% 
  ggplot(
    aes(
      x=cond2, 
      y=mean2
      ))+
  geom_col(aes(fill = cond2))+
  facet_wrap(~word_num)+
  labs(
    title = "Mean RT for words 2, 3, & 4",
    subtitle = "Comparing mean RT between 'break' & 'hit' verb conditions",
    y = "Mean RT (milisec.)",
    x = "Condition",
    fill = "Verb Type")
```

# Gennari and Poeppel 2003 Replication

The "gp" stimuli are from the study Gennari and Poeppel 2003. They found 
that eventive verbs had longer RTs than stative verbs. The verb is the fourth
word in these sentences. Was their effect replicated? Try the same steps. 

Gennari, S., & Poeppel, D. (2003). Processing correlates of lexical semantic complexity. Cognition, 89(1), B27–B41. <https://doi.org/10.1016/S0010-0277(03)00069-6>


## Summaries

```{r}
df_kable_gp <- df_sprt_joined %>% 
  filter(cond1 == 'gp' & word_num %in% c('4')) %>% 
  group_by(iphash, cond2, word_num) %>% 
  summarize(
    mean = mean(RT, na.rm=TRUE)
  ) %>% 
  ungroup() %>% 
  group_by(cond2) %>% 
  summarize(
    mean2 = mean(`mean`)
  )

kableExtra::kbl(df_kable_gp)
```

## Plots

```{r}
df_sprt_joined %>% 
  filter(cond1 == 'gp' & word_num %in% c('4')) %>% 
  group_by(iphash, cond2, word_num) %>% 
  summarize(
    mean = mean(RT, na.rm=TRUE),
    sd = sd(RT, na.rm=TRUE)
  ) %>% 
  ungroup() %>% 
  group_by(cond2) %>% 
  summarize(
    mean2 = mean(`mean`),
    sd2 = sd(`mean`)
  )%>% 
  ggplot(
    aes(
      x = cond2,
      y = mean2
    ))+
  geom_col(aes(fill = cond2))+
  geom_errorbar(aes(ymin=mean2-sd2, ymax=mean2+sd2), width = 0.05, position = position_dodge(.9))
```
